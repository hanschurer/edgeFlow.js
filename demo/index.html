<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>edgeFlow.js - Interactive Demo</title>
  <style>
    :root {
      --bg-primary: #0a0a0f;
      --bg-secondary: #12121a;
      --bg-card: #1a1a24;
      --text-primary: #e4e4e7;
      --text-secondary: #a1a1aa;
      --accent: #06b6d4;
      --accent-dim: #0891b2;
      --success: #22c55e;
      --warning: #f59e0b;
      --error: #ef4444;
      --border: #27272a;
    }

    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: 'SF Mono', 'Fira Code', 'JetBrains Mono', monospace;
      background: var(--bg-primary);
      color: var(--text-primary);
      min-height: 100vh;
      line-height: 1.6;
    }

    .container {
      max-width: 1200px;
      margin: 0 auto;
      padding: 2rem;
    }

    header {
      text-align: center;
      margin-bottom: 3rem;
      padding: 2rem;
      background: linear-gradient(135deg, var(--bg-secondary) 0%, var(--bg-card) 100%);
      border-radius: 16px;
      border: 1px solid var(--border);
    }

    h1 {
      font-size: 2.5rem;
      font-weight: 700;
      background: linear-gradient(135deg, var(--accent) 0%, #8b5cf6 100%);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      margin-bottom: 0.5rem;
    }

    .subtitle {
      color: var(--text-secondary);
      font-size: 1rem;
    }

    .grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(350px, 1fr));
      gap: 1.5rem;
      margin-bottom: 2rem;
    }

    .card {
      background: var(--bg-card);
      border: 1px solid var(--border);
      border-radius: 12px;
      padding: 1.5rem;
      transition: transform 0.2s, box-shadow 0.2s;
    }

    .card:hover {
      transform: translateY(-2px);
      box-shadow: 0 8px 32px rgba(6, 182, 212, 0.1);
    }

    .card-header {
      display: flex;
      align-items: center;
      gap: 0.75rem;
      margin-bottom: 1rem;
    }

    .card-icon {
      width: 40px;
      height: 40px;
      border-radius: 8px;
      display: flex;
      align-items: center;
      justify-content: center;
      font-size: 1.25rem;
      background: linear-gradient(135deg, var(--accent) 0%, #8b5cf6 100%);
    }

    .card-title {
      font-size: 1.125rem;
      font-weight: 600;
    }

    .status-list {
      display: flex;
      flex-direction: column;
      gap: 0.75rem;
    }

    .status-item {
      display: flex;
      justify-content: space-between;
      align-items: center;
      padding: 0.5rem 0.75rem;
      background: var(--bg-secondary);
      border-radius: 6px;
      font-size: 0.875rem;
    }

    .status-badge {
      padding: 0.25rem 0.75rem;
      border-radius: 9999px;
      font-size: 0.75rem;
      font-weight: 600;
    }

    .status-success { background: rgba(34, 197, 94, 0.2); color: var(--success); }
    .status-warning { background: rgba(245, 158, 11, 0.2); color: var(--warning); }
    .status-error { background: rgba(239, 68, 68, 0.2); color: var(--error); }
    .status-pending { background: rgba(161, 161, 170, 0.2); color: var(--text-secondary); }

    .test-section {
      background: var(--bg-card);
      border: 1px solid var(--border);
      border-radius: 12px;
      padding: 1.5rem;
      margin-bottom: 1.5rem;
    }

    .test-header {
      display: flex;
      justify-content: space-between;
      align-items: center;
      margin-bottom: 1rem;
    }

    .test-title {
      font-size: 1.25rem;
      font-weight: 600;
    }

    button {
      background: linear-gradient(135deg, var(--accent) 0%, var(--accent-dim) 100%);
      color: white;
      border: none;
      padding: 0.625rem 1.25rem;
      border-radius: 8px;
      font-family: inherit;
      font-size: 0.875rem;
      font-weight: 600;
      cursor: pointer;
      transition: opacity 0.2s, transform 0.2s;
    }

    button:hover {
      opacity: 0.9;
      transform: translateY(-1px);
    }

    button:disabled {
      opacity: 0.5;
      cursor: not-allowed;
      transform: none;
    }

    .btn-secondary {
      background: var(--bg-secondary);
      border: 1px solid var(--border);
    }

    input, textarea {
      width: 100%;
      padding: 0.75rem;
      background: var(--bg-secondary);
      border: 1px solid var(--border);
      border-radius: 8px;
      color: var(--text-primary);
      font-family: inherit;
      font-size: 0.875rem;
      margin-bottom: 1rem;
      transition: border-color 0.2s;
    }

    input:focus, textarea:focus {
      outline: none;
      border-color: var(--accent);
    }

    textarea {
      min-height: 100px;
      resize: vertical;
    }

    .output {
      background: var(--bg-primary);
      border: 1px solid var(--border);
      border-radius: 8px;
      padding: 1rem;
      font-size: 0.8125rem;
      overflow-x: auto;
      max-height: 300px;
      overflow-y: auto;
    }

    .output pre {
      white-space: pre-wrap;
      word-break: break-word;
    }

    .output .success { color: var(--success); }
    .output .error { color: var(--error); }
    .output .info { color: var(--accent); }
    .output .warn { color: var(--warning); }

    .metrics {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(120px, 1fr));
      gap: 1rem;
      margin-top: 1rem;
    }

    .metric {
      text-align: center;
      padding: 1rem;
      background: var(--bg-secondary);
      border-radius: 8px;
    }

    .metric-value {
      font-size: 1.5rem;
      font-weight: 700;
      color: var(--accent);
    }

    .metric-label {
      font-size: 0.75rem;
      color: var(--text-secondary);
      margin-top: 0.25rem;
    }

    .loader {
      display: inline-block;
      width: 16px;
      height: 16px;
      border: 2px solid var(--border);
      border-top-color: var(--accent);
      border-radius: 50%;
      animation: spin 0.8s linear infinite;
    }

    @keyframes spin {
      to { transform: rotate(360deg); }
    }

    .flex-gap {
      display: flex;
      gap: 0.75rem;
      flex-wrap: wrap;
    }

    footer {
      text-align: center;
      padding: 2rem;
      color: var(--text-secondary);
      font-size: 0.875rem;
    }

    footer a {
      color: var(--accent);
      text-decoration: none;
    }

    @media (max-width: 768px) {
      .container { padding: 1rem; }
      h1 { font-size: 1.75rem; }
      .grid { grid-template-columns: 1fr; }
    }
  </style>
  <!-- ONNX Runtime is loaded automatically by edgeFlow when needed -->
</head>
<body>
  <div class="container">
    <header>
      <h1>‚ö° edgeFlow.js</h1>
      <p class="subtitle">Lightweight Browser ML Inference Framework</p>
    </header>

    <!-- Runtime Detection -->
    <div class="grid">
      <div class="card">
        <div class="card-header">
          <div class="card-icon">üñ•Ô∏è</div>
          <div class="card-title">Runtime Detection</div>
        </div>
        <div class="status-list" id="runtime-status">
          <div class="status-item">
            <span>WebGPU</span>
            <span class="status-badge status-pending" id="webgpu-status">Checking...</span>
          </div>
          <div class="status-item">
            <span>WebNN</span>
            <span class="status-badge status-pending" id="webnn-status">Checking...</span>
          </div>
          <div class="status-item">
            <span>WASM</span>
            <span class="status-badge status-pending" id="wasm-status">Checking...</span>
          </div>
        </div>
      </div>

      <div class="card">
        <div class="card-header">
          <div class="card-icon">üìä</div>
          <div class="card-title">Memory Stats</div>
        </div>
        <div class="status-list" id="memory-stats">
          <div class="status-item">
            <span>Allocated</span>
            <span id="mem-allocated">0 MB</span>
          </div>
          <div class="status-item">
            <span>Peak</span>
            <span id="mem-peak">0 MB</span>
          </div>
          <div class="status-item">
            <span>Tensors</span>
            <span id="mem-tensors">0</span>
          </div>
        </div>
        <button onclick="refreshMemoryStats()" style="margin-top: 1rem; width: 100%;">
          Refresh Stats
        </button>
      </div>
    </div>

    <!-- Tensor Operations Test -->
    <div class="test-section">
      <div class="test-header">
        <h3 class="test-title">üßÆ Tensor Operations</h3>
        <button onclick="runTensorTests()">Run Tests</button>
      </div>
      <div class="output" id="tensor-output">
        <pre>Click "Run Tests" to test tensor operations...</pre>
      </div>
    </div>

    <!-- Model Loading Test -->
    <div class="test-section">
      <div class="test-header">
        <h3 class="test-title">üì¶ Load Real ONNX Model</h3>
      </div>
      <input type="text" id="model-url" placeholder="Enter ONNX model URL..." 
             value="https://huggingface.co/Xenova/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/onnx/model_quantized.onnx">
      <div class="flex-gap">
        <button onclick="loadRealModel()">Load Model</button>
        <button class="btn-secondary" onclick="testLoadedModel()">Test Inference</button>
      </div>
      <div class="output" id="model-output" style="margin-top: 1rem;">
        <pre><span class="info">Available Models (from Hugging Face):</span>

‚Ä¢ DistilBERT Sentiment: 
  https://huggingface.co/Xenova/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/onnx/model_quantized.onnx

‚Ä¢ MobileNet Image Classification:
  https://huggingface.co/Xenova/mobilenet_v2_1.0_224/resolve/main/onnx/model_quantized.onnx

‚Ä¢ BERT Base:
  https://huggingface.co/Xenova/bert-base-uncased/resolve/main/onnx/model_quantized.onnx

Click "Load Model" to download and initialize a real ONNX model.</pre>
      </div>
    </div>

    <!-- Text Classification Test -->
    <div class="test-section">
      <div class="test-header">
        <h3 class="test-title">üìù Text Classification</h3>
      </div>
      <input type="text" id="text-input" placeholder="Enter text to classify..." value="I love this product! It's amazing!">
      <div class="flex-gap">
        <button onclick="runTextClassification()">Classify</button>
        <button class="btn-secondary" onclick="runBatchClassification()">Batch Test</button>
      </div>
      <div class="output" id="text-output" style="margin-top: 1rem;">
        <pre>Load a model first, then enter text and click "Classify"...</pre>
      </div>
    </div>

    <!-- Feature Extraction Test -->
    <div class="test-section">
      <div class="test-header">
        <h3 class="test-title">üîç Feature Extraction</h3>
      </div>
      <textarea id="feature-input" placeholder="Enter text for feature extraction...">Machine learning is transforming how we build software applications.</textarea>
      <button onclick="runFeatureExtraction()">Extract Features</button>
      <div class="output" id="feature-output" style="margin-top: 1rem;">
        <pre>Enter text and click "Extract Features"...</pre>
      </div>
    </div>

    <!-- Concurrent Execution Test -->
    <div class="test-section">
      <div class="test-header">
        <h3 class="test-title">‚ö° Concurrent Execution</h3>
        <button onclick="runConcurrencyTest()">Run Concurrency Test</button>
      </div>
      <p style="color: var(--text-secondary); font-size: 0.875rem; margin-bottom: 1rem;">
        Tests parallel execution of multiple pipeline tasks
      </p>
      <div class="output" id="concurrency-output">
        <pre>Click to test concurrent execution...</pre>
      </div>
      <div class="metrics" id="concurrency-metrics" style="display: none;">
        <div class="metric">
          <div class="metric-value" id="metric-total">-</div>
          <div class="metric-label">Total Time</div>
        </div>
        <div class="metric">
          <div class="metric-value" id="metric-tasks">-</div>
          <div class="metric-label">Tasks</div>
        </div>
        <div class="metric">
          <div class="metric-value" id="metric-avg">-</div>
          <div class="metric-label">Avg/Task</div>
        </div>
      </div>
    </div>

    <!-- Scheduler Test -->
    <div class="test-section">
      <div class="test-header">
        <h3 class="test-title">üìã Task Scheduler</h3>
        <button onclick="runSchedulerTest()">Test Scheduler</button>
      </div>
      <div class="output" id="scheduler-output">
        <pre>Click to test the task scheduler...</pre>
      </div>
    </div>

    <!-- Memory Management Test -->
    <div class="test-section">
      <div class="test-header">
        <h3 class="test-title">üíæ Memory Management</h3>
        <div class="flex-gap">
          <button onclick="runMemoryTest()">Allocate Tensors</button>
          <button class="btn-secondary" onclick="runMemoryCleanup()">Cleanup</button>
        </div>
      </div>
      <div class="output" id="memory-output">
        <pre>Test memory allocation and cleanup...</pre>
      </div>
    </div>

    <footer>
      <p>
        edgeFlow.js v0.1.0 | 
        <a href="https://github.com/edgeflow/edgeflow.js" target="_blank">GitHub</a>
      </p>
    </footer>
  </div>

  <script type="module">
    // Import edgeFlow browser bundle
    import * as edgeFlow from '../dist/edgeflow.browser.js';
    
    // Make it globally available for onclick handlers
    window.edgeFlow = edgeFlow;

    // Store references
    let textPipeline = null;
    let featurePipeline = null;
    let testTensors = [];
    let loadedModel = null;

    // =========================================================================
    // Model Configuration - Real ONNX Models from Hugging Face
    // =========================================================================
    // 
    // Using real quantized models for sentiment analysis
    // These models are from the Xenova/transformers.js collection
    //
    const MODEL_CONFIG = {
      // Sentiment Analysis model (DistilBERT fine-tuned on SST-2)
      sentimentAnalysis: 'https://huggingface.co/Xenova/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/onnx/model_quantized.onnx',
      
      // Feature Extraction model (all-MiniLM-L6-v2)
      featureExtraction: 'https://huggingface.co/Xenova/all-MiniLM-L6-v2/resolve/main/onnx/model_quantized.onnx',
      
      // Image classification (MobileNet)
      imageClassification: 'https://huggingface.co/Xenova/mobilenet_v2_1.0_224/resolve/main/onnx/model_quantized.onnx',
    };

    // Initialize on load
    window.addEventListener('DOMContentLoaded', async () => {
      await checkRuntimes();
      refreshMemoryStats();
    });

    // Check available runtimes
    async function checkRuntimes() {
      try {
        const runtimes = await edgeFlow.getAvailableRuntimes();
        
        updateStatus('webgpu-status', runtimes.get('webgpu'));
        updateStatus('webnn-status', runtimes.get('webnn'));
        updateStatus('wasm-status', runtimes.get('wasm'));
      } catch (error) {
        console.error('Runtime detection failed:', error);
        updateStatus('webgpu-status', false);
        updateStatus('webnn-status', false);
        updateStatus('wasm-status', false);
      }
    }

    function updateStatus(id, available) {
      const el = document.getElementById(id);
      if (available) {
        el.textContent = 'Available';
        el.className = 'status-badge status-success';
      } else {
        el.textContent = 'Unavailable';
        el.className = 'status-badge status-error';
      }
    }

    // Refresh memory stats
    window.refreshMemoryStats = function() {
      try {
        const stats = edgeFlow.getMemoryStats();
        document.getElementById('mem-allocated').textContent = formatBytes(stats.allocated);
        document.getElementById('mem-peak').textContent = formatBytes(stats.peak);
        document.getElementById('mem-tensors').textContent = stats.tensorCount;
      } catch (error) {
        console.error('Memory stats error:', error);
      }
    };

    function formatBytes(bytes) {
      if (bytes === 0) return '0 B';
      const k = 1024;
      const sizes = ['B', 'KB', 'MB', 'GB'];
      const i = Math.floor(Math.log(bytes) / Math.log(k));
      return parseFloat((bytes / Math.pow(k, i)).toFixed(2)) + ' ' + sizes[i];
    }

    // =========================================================================
    // Model Loading Functions
    // =========================================================================
    
    window.loadRealModel = async function() {
      const output = document.getElementById('model-output');
      const modelUrl = document.getElementById('model-url').value;

      if (!modelUrl.trim()) {
        output.innerHTML = '<pre><span class="warn">Please enter a model URL</span></pre>';
        return;
      }

      output.innerHTML = '<pre><span class="loader"></span> Downloading and loading model...\n\nThis may take a moment for larger models...</pre>';

      try {
        const startTime = performance.now();
        
        // Load model using edgeFlow
        loadedModel = await edgeFlow.loadModel(modelUrl, {
          runtime: 'wasm',  // Use ONNX Runtime backend
        });
        
        const loadTime = ((performance.now() - startTime) / 1000).toFixed(2);

        output.innerHTML = `<pre><span class="success">‚úì Model loaded successfully!</span>

<span class="info">Model Info:</span>
  ID: ${loadedModel.id}
  Name: ${loadedModel.metadata.name}
  Size: ${formatBytes(loadedModel.metadata.sizeBytes)}
  Format: ${loadedModel.metadata.format}
  Quantization: ${loadedModel.metadata.quantization}
  
<span class="info">Inputs:</span>
${loadedModel.metadata.inputs.map(i => `  ‚Ä¢ ${i.name}: ${i.dtype}`).join('\n')}

<span class="info">Outputs:</span>
${loadedModel.metadata.outputs.map(o => `  ‚Ä¢ ${o.name}: ${o.dtype}`).join('\n')}

Load time: ${loadTime}s

<span class="info">You can now use "Test Inference" to run the model.</span></pre>`;

        refreshMemoryStats();
      } catch (error) {
        output.innerHTML = `<pre><span class="error">Error loading model:</span>
${error.message}

<span class="warn">Note: Some models require CORS headers. Try models from Hugging Face
that support cross-origin requests.</span></pre>`;
        console.error('Model loading error:', error);
      }
    };

    window.testLoadedModel = async function() {
      const output = document.getElementById('model-output');

      if (!loadedModel) {
        output.innerHTML = '<pre><span class="warn">Please load a model first</span></pre>';
        return;
      }

      output.innerHTML = '<pre><span class="loader"></span> Running inference...</pre>';

      try {
        // Get model input names to determine what inputs are needed
        const inputNames = loadedModel.metadata.inputs.map(i => i.name);
        output.innerHTML = `<pre><span class="loader"></span> Running inference...
Model expects inputs: ${inputNames.join(', ')}</pre>`;

        // Create inputs based on what the model expects
        const seqLen = 128; // Sequence length
        const inputs = [];
        
        for (const inputSpec of loadedModel.metadata.inputs) {
          const name = inputSpec.name;
          let inputData = new Array(seqLen).fill(0);
          
          if (name === 'input_ids' || name.includes('input')) {
            // Token IDs (integers)
            inputData[0] = 101; // [CLS] token
            inputData[1] = 2054; // "hello"
            inputData[2] = 102;  // [SEP] token
          } else if (name === 'attention_mask' || name.includes('mask')) {
            // Attention mask: 1 for real tokens, 0 for padding
            inputData[0] = 1; inputData[1] = 1; inputData[2] = 1;
          }
          // token_type_ids stays all zeros
          
          // Use int64 for BERT-style models
          const tensor = edgeFlow.tensor(inputData, [1, seqLen], 'int64');
          inputs.push(tensor);
        }

        const startTime = performance.now();
        const outputs = await edgeFlow.runInference(loadedModel, inputs);
        const inferenceTime = (performance.now() - startTime).toFixed(2);

        let outputInfo = outputs.map((t, i) => {
          const data = t.toArray();
          const preview = data.slice(0, 10).map(x => x.toFixed(4));
          return `Output ${i}: shape=[${t.shape}], values=[${preview.join(', ')}${data.length > 10 ? '...' : ''}]`;
        }).join('\n');

        output.innerHTML = `<pre><span class="success">‚úì Inference completed!</span>

<span class="info">Model Inputs:</span>
${inputNames.map(n => `  ‚Ä¢ ${n}`).join('\n')}

<span class="info">Results:</span>
${outputInfo}

Inference time: ${inferenceTime}ms</pre>`;

        // Cleanup
        inputs.forEach(t => t.dispose());
        outputs.forEach(t => t.dispose());
        refreshMemoryStats();
      } catch (error) {
        output.innerHTML = `<pre><span class="error">Inference error:</span>
${error.message}

<span class="warn">The model may require specific input formats.
Check the model's documentation for expected inputs.</span></pre>`;
        console.error('Inference error:', error);
      }
    };

    // Tensor operations test
    window.runTensorTests = function() {
      const output = document.getElementById('tensor-output');
      let results = [];

      try {
        // Test tensor creation
        results.push('<span class="info">Testing tensor operations...</span>\n');
        
        const a = edgeFlow.tensor([[1, 2], [3, 4]]);
        results.push(`‚úì Created 2x2 tensor: shape=[${a.shape}]`);

        const zeros = edgeFlow.zeros([3, 3]);
        results.push(`‚úì Created zeros tensor: shape=[${zeros.shape}]`);

        const ones = edgeFlow.ones([2, 4]);
        results.push(`‚úì Created ones tensor: shape=[${ones.shape}]`);

        const rand = edgeFlow.random([10]);
        results.push(`‚úì Created random tensor: shape=[${rand.shape}]`);

        // Test operations
        const b = edgeFlow.tensor([[5, 6], [7, 8]]);
        const sum = edgeFlow.add(a, b);
        results.push(`‚úì Addition: [${a.toArray().slice(0,4)}] + [${b.toArray().slice(0,4)}] = [${sum.toArray().slice(0,4)}]`);

        const product = edgeFlow.mul(a, b);
        results.push(`‚úì Multiplication: element-wise product computed`);

        const c = edgeFlow.tensor([[1, 2, 3], [4, 5, 6]]);
        const d = edgeFlow.tensor([[7, 8], [9, 10], [11, 12]]);
        const matmulResult = edgeFlow.matmul(c, d);
        results.push(`‚úì Matrix multiplication: (2x3) @ (3x2) = shape=[${matmulResult.shape}]`);

        // Test activations
        const logits = edgeFlow.tensor([1, 2, 3, 4]);
        const probs = edgeFlow.softmax(logits);
        results.push(`‚úì Softmax: [${logits.toArray()}] ‚Üí [${probs.toArray().map(x => x.toFixed(3))}]`);

        const negValues = edgeFlow.tensor([-1, 0, 1, 2]);
        const reluResult = edgeFlow.relu(negValues);
        results.push(`‚úì ReLU: [${negValues.toArray()}] ‚Üí [${reluResult.toArray()}]`);

        // Cleanup
        [a, b, sum, product, c, d, matmulResult, logits, probs, negValues, reluResult, zeros, ones, rand].forEach(t => t.dispose());

        results.push('\n<span class="success">All tensor tests passed! ‚úì</span>');
      } catch (error) {
        results.push(`\n<span class="error">Error: ${error.message}</span>`);
      }

      output.innerHTML = `<pre>${results.join('\n')}</pre>`;
      refreshMemoryStats();
    };

    // Text classification - uses the loaded model
    window.runTextClassification = async function() {
      const output = document.getElementById('text-output');
      const input = document.getElementById('text-input').value;

      if (!loadedModel) {
        output.innerHTML = '<pre><span class="warn">‚ö†Ô∏è Please load a model first using the "Load Real ONNX Model" section above.</span></pre>';
        return;
      }

      if (!input.trim()) {
        output.innerHTML = '<pre><span class="warn">Please enter some text</span></pre>';
        return;
      }

      output.innerHTML = '<pre><span class="loader"></span> Running inference with loaded model...</pre>';

      try {
        // Simple tokenization (space-split, convert to token IDs)
        const tokens = input.toLowerCase().split(/\s+/);
        const maxLen = 128;
        
        // Create inputs based on model requirements
        const inputs = [];
        const numTokens = Math.min(tokens.length + 2, maxLen); // +2 for [CLS] and [SEP]
        
        for (const inputSpec of loadedModel.metadata.inputs) {
          const name = inputSpec.name;
          let inputData = new Array(maxLen).fill(0);
          
          if (name === 'input_ids' || name.includes('input')) {
            inputData[0] = 101; // [CLS]
            tokens.slice(0, maxLen - 2).forEach((token, i) => {
              // Simple hash to get "token ID"
              inputData[i + 1] = Math.abs(token.split('').reduce((a, c) => a + c.charCodeAt(0), 0)) % 30000;
            });
            inputData[numTokens - 1] = 102; // [SEP]
          } else if (name === 'attention_mask' || name.includes('mask')) {
            for (let i = 0; i < numTokens; i++) inputData[i] = 1;
          }
          // token_type_ids and others stay all zeros
          
          inputs.push(edgeFlow.tensor(inputData, [1, maxLen], 'int64'));
        }
        
        const start = performance.now();
        const outputs = await edgeFlow.runInference(loadedModel, inputs);
        const time = (performance.now() - start).toFixed(2);

        // Get output and interpret as sentiment
        const outputData = outputs[0].toArray();
        const score = outputData.length >= 2 
          ? Math.exp(outputData[1]) / (Math.exp(outputData[0]) + Math.exp(outputData[1]))
          : outputData[0] > 0.5 ? outputData[0] : 1 - outputData[0];
        const label = score > 0.5 ? 'POSITIVE' : 'NEGATIVE';

        output.innerHTML = `<pre><span class="info">Input:</span> "${input}"

<span class="success">Model Output:</span>
  Raw output: [${outputData.slice(0, 5).map(x => x.toFixed(4)).join(', ')}${outputData.length > 5 ? '...' : ''}]
  Interpreted: ${label} (${(score * 100).toFixed(1)}%)
  Inference time: ${time}ms
  
<span class="warn">Note: This demo uses simplified tokenization.
Real applications should use a proper tokenizer.</span></pre>`;

        // Cleanup
        inputs.forEach(t => t.dispose());
        outputs.forEach(t => t.dispose());
      } catch (error) {
        output.innerHTML = `<pre><span class="error">Error: ${error.message}</span></pre>`;
      }

      refreshMemoryStats();
    };

    // Batch classification
    window.runBatchClassification = async function() {
      const output = document.getElementById('text-output');

      if (!loadedModel) {
        output.innerHTML = '<pre><span class="warn">‚ö†Ô∏è Please load a model first using the "Load Real ONNX Model" section above.</span></pre>';
        return;
      }

      const texts = [
        "I love this product!",
        "This is terrible, I hate it.",
        "It's okay, nothing special.",
        "Amazing experience, highly recommend!",
        "Worst purchase ever, total waste."
      ];

      output.innerHTML = '<pre><span class="loader"></span> Running batch inference...</pre>';

      try {
        const start = performance.now();
        const results = [];

        for (const text of texts) {
          const tokens = text.toLowerCase().split(/\s+/);
          const maxLen = 128;
          const numTokens = Math.min(tokens.length + 2, maxLen);
          
          const inputs = [];
          for (const inputSpec of loadedModel.metadata.inputs) {
            const name = inputSpec.name;
            let inputData = new Array(maxLen).fill(0);
            
            if (name === 'input_ids' || name.includes('input')) {
              inputData[0] = 101;
              tokens.slice(0, maxLen - 2).forEach((token, i) => {
                inputData[i + 1] = Math.abs(token.split('').reduce((a, c) => a + c.charCodeAt(0), 0)) % 30000;
              });
              inputData[numTokens - 1] = 102;
            } else if (name === 'attention_mask' || name.includes('mask')) {
              for (let i = 0; i < numTokens; i++) inputData[i] = 1;
            }
            inputs.push(edgeFlow.tensor(inputData, [1, maxLen], 'int64'));
          }

          const outputs = await edgeFlow.runInference(loadedModel, inputs);
          const outputData = outputs[0].toArray();
          
          const score = outputData.length >= 2 
            ? Math.exp(outputData[1]) / (Math.exp(outputData[0]) + Math.exp(outputData[1]))
            : outputData[0] > 0.5 ? outputData[0] : 1 - outputData[0];
          
          results.push({ label: score > 0.5 ? 'positive' : 'negative', score });
          
          inputs.forEach(t => t.dispose());
          outputs.forEach(t => t.dispose());
        }

        const time = (performance.now() - start).toFixed(2);

        let resultText = `<span class="info">Batch Inference (${texts.length} items)</span>\n\n`;
        results.forEach((r, i) => {
          const emoji = r.label === 'positive' ? 'üòä' : 'üòû';
          resultText += `${emoji} "${texts[i]}"\n   ‚Üí ${r.label} (${(r.score * 100).toFixed(1)}%)\n\n`;
        });
        resultText += `<span class="success">Total time: ${time}ms (${(time/texts.length).toFixed(2)}ms/item)</span>`;

        output.innerHTML = `<pre>${resultText}</pre>`;
      } catch (error) {
        output.innerHTML = `<pre><span class="error">Error: ${error.message}</span></pre>`;
      }

      refreshMemoryStats();
    };

    // Feature extraction - uses the loaded model
    window.runFeatureExtraction = async function() {
      const output = document.getElementById('feature-output');
      const input = document.getElementById('feature-input').value;

      if (!loadedModel) {
        output.innerHTML = '<pre><span class="warn">‚ö†Ô∏è Please load a model first using the "Load Real ONNX Model" section above.</span></pre>';
        return;
      }

      if (!input.trim()) {
        output.innerHTML = '<pre><span class="warn">Please enter some text</span></pre>';
        return;
      }

      output.innerHTML = '<pre><span class="loader"></span> Extracting features...</pre>';

      try {
        const tokens = input.toLowerCase().split(/\s+/);
        const maxLen = 128;
        const numTokens = Math.min(tokens.length + 2, maxLen);
        
        const inputs = [];
        for (const inputSpec of loadedModel.metadata.inputs) {
          const name = inputSpec.name;
          let inputData;
          
          if (name === 'input_ids' || name.includes('input')) {
            inputData = new Array(maxLen).fill(0);
            inputData[0] = 101;
            tokens.slice(0, maxLen - 2).forEach((token, i) => {
              inputData[i + 1] = Math.abs(token.split('').reduce((a, c) => a + c.charCodeAt(0), 0)) % 30000;
            });
            inputData[numTokens - 1] = 102;
          } else if (name === 'attention_mask' || name.includes('mask')) {
            inputData = new Array(maxLen).fill(0);
            for (let i = 0; i < numTokens; i++) inputData[i] = 1;
          } else {
            inputData = new Array(maxLen).fill(0);
          }
          inputs.push(edgeFlow.tensor(inputData, [1, maxLen], 'int64'));
        }

        const start = performance.now();
        const outputs = await edgeFlow.runInference(loadedModel, inputs);
        const time = (performance.now() - start).toFixed(2);

        const embeddings = outputs[0].toArray();
        const preview = embeddings.slice(0, 10).map(x => x.toFixed(4));

        output.innerHTML = `<pre><span class="info">Feature Extraction Result</span>

Output dimension: ${embeddings.length}
First 10 values: [${preview.join(', ')}${embeddings.length > 10 ? '...' : ''}]

L2 norm: ${Math.sqrt(embeddings.reduce((a, b) => a + b*b, 0)).toFixed(4)}
Inference time: ${time}ms</pre>`;

        // Cleanup
        inputs.forEach(t => t.dispose());
        outputs.forEach(t => t.dispose());
      } catch (error) {
        output.innerHTML = `<pre><span class="error">Error: ${error.message}</span></pre>`;
      }

      refreshMemoryStats();
    };

    // Concurrency test - uses loaded model
    window.runConcurrencyTest = async function() {
      const output = document.getElementById('concurrency-output');
      const metricsDiv = document.getElementById('concurrency-metrics');

      if (!loadedModel) {
        output.innerHTML = '<pre><span class="warn">‚ö†Ô∏è Please load a model first using the "Load Real ONNX Model" section above.</span></pre>';
        metricsDiv.style.display = 'none';
        return;
      }

      output.innerHTML = '<pre><span class="loader"></span> Running concurrent inference tasks...</pre>';
      metricsDiv.style.display = 'none';

      try {
        const texts = [
          'I love this!',
          'This is terrible.',
          'Amazing product!',
          'Worst ever.',
          'Pretty good overall.',
          'Not worth it.',
          'Highly recommend!',
          'Total disappointment.'
        ];

        // Helper function for single inference
        async function runSingleInference(text) {
          const tokens = text.toLowerCase().split(/\s+/);
          const maxLen = 128;
          const numTokens = Math.min(tokens.length + 2, maxLen);
          
          const inputs = [];
          for (const inputSpec of loadedModel.metadata.inputs) {
            const name = inputSpec.name;
            let inputData = new Array(maxLen).fill(0);
            
            if (name === 'input_ids' || name.includes('input')) {
              inputData[0] = 101;
              tokens.slice(0, maxLen - 2).forEach((token, i) => {
                inputData[i + 1] = Math.abs(token.split('').reduce((a, c) => a + c.charCodeAt(0), 0)) % 30000;
              });
              inputData[numTokens - 1] = 102;
            } else if (name === 'attention_mask' || name.includes('mask')) {
              for (let i = 0; i < numTokens; i++) inputData[i] = 1;
            }
            inputs.push(edgeFlow.tensor(inputData, [1, maxLen], 'int64'));
          }
          
          const outputs = await edgeFlow.runInference(loadedModel, inputs);
          const outputData = outputs[0].toArray();
          
          const score = outputData.length >= 2 
            ? Math.exp(outputData[1]) / (Math.exp(outputData[0]) + Math.exp(outputData[1]))
            : outputData[0] > 0.5 ? outputData[0] : 1 - outputData[0];
          
          inputs.forEach(t => t.dispose());
          outputs.forEach(t => t.dispose());
          
          return { label: score > 0.5 ? 'positive' : 'negative', score };
        }

        // Run all tasks concurrently
        const start = performance.now();
        const results = await Promise.all(texts.map(text => runSingleInference(text)));
        const totalTime = performance.now() - start;

        let resultText = '<span class="info">Concurrent Inference Results</span>\n\n';
        results.forEach((r, i) => {
          const emoji = r.label === 'positive' ? 'üòä' : 'üòû';
          resultText += `${emoji} Task ${i+1}: "${texts[i]}" ‚Üí ${r.label}\n`;
        });
        resultText += `\n<span class="success">All ${texts.length} tasks completed concurrently!</span>`;

        output.innerHTML = `<pre>${resultText}</pre>`;

        // Show metrics
        document.getElementById('metric-total').textContent = totalTime.toFixed(0) + 'ms';
        document.getElementById('metric-tasks').textContent = texts.length;
        document.getElementById('metric-avg').textContent = (totalTime / texts.length).toFixed(1) + 'ms';
        metricsDiv.style.display = 'grid';
      } catch (error) {
        output.innerHTML = `<pre><span class="error">Error: ${error.message}</span></pre>`;
      }

      refreshMemoryStats();
    };

    // Scheduler test
    window.runSchedulerTest = async function() {
      const output = document.getElementById('scheduler-output');
      output.innerHTML = '<pre><span class="loader"></span> Testing scheduler...</pre>';

      try {
        const scheduler = edgeFlow.getScheduler();
        let results = ['<span class="info">Scheduler Test</span>\n'];

        // Schedule some tasks
        const task1 = scheduler.schedule('model-a', async () => {
          await new Promise(r => setTimeout(r, 100));
          return 'Task 1 done';
        }, 'high');

        const task2 = scheduler.schedule('model-b', async () => {
          await new Promise(r => setTimeout(r, 50));
          return 'Task 2 done';
        }, 'normal');

        const task3 = scheduler.schedule('model-a', async () => {
          await new Promise(r => setTimeout(r, 75));
          return 'Task 3 done';
        }, 'low');

        results.push(`Scheduled 3 tasks with different priorities\n`);

        // Wait for all tasks
        const [r1, r2, r3] = await Promise.all([
          task1.wait(),
          task2.wait(),
          task3.wait()
        ]);

        results.push(`‚úì ${r1}`);
        results.push(`‚úì ${r2}`);
        results.push(`‚úì ${r3}`);

        // Get stats
        const stats = scheduler.getStats();
        results.push(`\n<span class="info">Scheduler Stats:</span>`);
        results.push(`  Total tasks: ${stats.totalTasks}`);
        results.push(`  Completed: ${stats.completedTasks}`);
        results.push(`  Running: ${stats.runningTasks}`);
        results.push(`  Pending: ${stats.pendingTasks}`);

        results.push(`\n<span class="success">Scheduler test passed!</span>`);
        output.innerHTML = `<pre>${results.join('\n')}</pre>`;
      } catch (error) {
        output.innerHTML = `<pre><span class="error">Error: ${error.message}</span></pre>`;
      }
    };

    // Memory test
    window.runMemoryTest = async function() {
      const output = document.getElementById('memory-output');
      output.innerHTML = '<pre><span class="loader"></span> Allocating tensors...</pre>';

      try {
        let results = ['<span class="info">Memory Allocation Test</span>\n'];

        const beforeStats = edgeFlow.getMemoryStats();
        results.push(`Before: ${formatBytes(beforeStats.allocated)} allocated, ${beforeStats.tensorCount} tensors`);

        // Allocate several tensors
        for (let i = 0; i < 10; i++) {
          const t = edgeFlow.random([100, 100]);
          testTensors.push(t);
        }

        const afterStats = edgeFlow.getMemoryStats();
        results.push(`After: ${formatBytes(afterStats.allocated)} allocated, ${afterStats.tensorCount} tensors`);
        results.push(`\n<span class="success">Created 10 tensors (100x100 each)</span>`);
        results.push(`<span class="warn">Click "Cleanup" to release memory</span>`);

        output.innerHTML = `<pre>${results.join('\n')}</pre>`;
        refreshMemoryStats();
      } catch (error) {
        output.innerHTML = `<pre><span class="error">Error: ${error.message}</span></pre>`;
      }
    };

    // Memory cleanup
    window.runMemoryCleanup = function() {
      const output = document.getElementById('memory-output');

      try {
        let results = ['<span class="info">Memory Cleanup</span>\n'];

        const beforeStats = edgeFlow.getMemoryStats();
        results.push(`Before cleanup: ${formatBytes(beforeStats.allocated)}, ${beforeStats.tensorCount} tensors`);

        // Dispose all test tensors
        testTensors.forEach(t => {
          if (!t.isDisposed) t.dispose();
        });
        testTensors = [];

        // Force GC
        edgeFlow.gc();

        const afterStats = edgeFlow.getMemoryStats();
        results.push(`After cleanup: ${formatBytes(afterStats.allocated)}, ${afterStats.tensorCount} tensors`);
        results.push(`\n<span class="success">Memory cleaned up successfully!</span>`);

        output.innerHTML = `<pre>${results.join('\n')}</pre>`;
        refreshMemoryStats();
      } catch (error) {
        output.innerHTML = `<pre><span class="error">Error: ${error.message}</span></pre>`;
      }
    };
  </script>
</body>
</html>
